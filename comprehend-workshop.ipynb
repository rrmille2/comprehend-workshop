{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Textract and Amazon Comprehend AI Services\n",
    "### Example on extracting insights from a PDF Document\n",
    "\n",
    "\n",
    "## Contents \n",
    "1. [Background](#Background)\n",
    "1. [Notes and Configuration](#Notes-and-Configuration)\n",
    "1. [Amazon Textract](#Amazon-Textract)\n",
    "1. [Amazon Comprehend](#Amazon-Comprehend)\n",
    "1. [Key Phrase Extraction](#Key-Phrase-Extraction)\n",
    "1. [Sentiment Analysis](#Sentiment-Analysis)\n",
    "1. [Entity Recognition](#Entity-Recognition)\n",
    "1. [PII Entity Recognition](#PII-Entity-Recognition)\n",
    "1. [Topic Modeling](#Topic-Modeling)\n",
    "\n",
    "  \n",
    "## Background\n",
    "The goal of this exercise is to learn some insights from an existing PDF document. This is done by using Amazon Textract to extract the text from the document. This text is then analyzed by several Amazon Comprehend services to produce some insights about the document.  \n",
    "\n",
    "#### Notes and Configuration\n",
    "* Kernel `Python 3 (Data Science)` works well with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Set some variables that will be used throughout this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'us-east-1'\n",
    "\n",
    "sess = sm.Session()\n",
    "s3bucket = sess.default_bucket()    \n",
    "s3prefix = 'comprehend'\n",
    "\n",
    "# this is where the various analysis results files will be stored on the local file system of this SageMaker instance\n",
    "results_dir = './results'\n",
    "!mkdir -p $results_dir\n",
    "\n",
    "# this is the IAM Role that defines which permissions this SageMaker instance has\n",
    "sm_execution_role = sm.get_execution_role()\n",
    "print('sagemaker execution role: ', sm_execution_role)\n",
    "print('s3 bucket:', s3bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAM Roles and Permissions:\n",
    "\n",
    "Within SageMaker Studio, each SageMaker User has an IAM Role known as the `SageMaker Execution Role`. Each Notebook for this user will run with this Role and the Permissions specified by this Role. The name of this Role can be found in the Details section of each SageMaker User in the AWS Console.\n",
    "\n",
    "For the code which runs in this notebook, the `SageMaker Execution Role` needs additional permissions to allow it to use Amazon Textract and Amazon Comprehend. In the AWS Console, navigate to the IAM service and add these two services to your SageMaker Execution Role:\n",
    "- AmazonTextractFullAccess\n",
    "- AmazonComprehendFullAccess\n",
    "\n",
    "Also, an Amazon Comprehend service Role needs to be created to grant Amazon Comprehend read access to your input data.  \n",
    "When creating this new Role, the default Policies are sufficient (i.e., no other Policies need to be added/modified).  \n",
    "In our example, we are creating a Role with the name `myComprehendServiceRole`\n",
    "\n",
    "Lastly, the `SageMaker Execution Role` must be allowed to Pass the Comprehend Service Role. To allow this, you must attach a Policy to the `SageMaker Execution Role`. Below, the Resource entry is the ARN of the Comprehend service Role which you created. You can either create this as a new Policy and attach it or add it as an in-line Policy.  \n",
    "In our example, we are creating a Role with the name `ComprehendDataAccessForSageMaker`\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:GetRole\",\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:iam::810190279255:role/myComprehendServiceRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to the ARN of the Role you created\n",
    "comprehend_role = 'arn:aws:iam::662559257807:role/ComprehendDataAccessForSageMaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Textract\n",
    "Amazon Textract is a machine learning service that automatically extracts text, handwriting and data from scanned documents that goes beyond simple optical character recognition (OCR) to identify, understand, and extract data from forms and tables.  \n",
    "  \n",
    "In the next few cells the following steps will be performed:\n",
    "1. A specified PDF document will be uploaded to Amazon S3 to be analyzed by Amazon Textract.  \n",
    "1. The result of this analysis is a JSON file with each element containing details about a specific instance of text in the PDF.  \n",
    "1. This JSON file is copied from S3 to this local SageMaker instance.  \n",
    "1. The JSON file is then read and post-processed to produce a text file with one tweet (or other social media post) per line.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a boto3 session\n",
    "# this session will be used for the remainder of this notebook\n",
    "session = boto3.Session(region_name=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Textract Job\n",
    "textract_src_filename = 'amazon-press-release.png'\n",
    "textract_src_filename = 'police-report.pdf'\n",
    "\n",
    "# upload the source document to S3 for Textract to access\n",
    "!aws s3 cp data/$textract_src_filename s3://$s3bucket/$s3prefix/$textract_src_filename\n",
    "\n",
    "textract_client = session.client('textract')\n",
    "response = textract_client.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "    'S3Object': {\n",
    "        'Bucket': s3bucket,\n",
    "        'Name': f'{s3prefix}/{textract_src_filename}'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "JobId = response[\"JobId\"]\n",
    "print('JobId: %s' % (JobId))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = textract_client.get_document_text_detection(JobId=JobId)\n",
    "print(response['JobStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response['JobStatus'] != 'SUCCEEDED':\n",
    "    raise\n",
    "    \n",
    "pages = []\n",
    "while(True):\n",
    "    pages.append(response)\n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "        response = textract_client.get_document_text_detection(JobId=JobId, NextToken=nextToken)\n",
    "\n",
    "    if nextToken == None:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "# iterate through the Textract JSON response, looking for the LINE and WORD entries\n",
    "for page in pages:\n",
    "    for blk in page['Blocks']:\n",
    "        if blk['BlockType'] in ['LINE']:\n",
    "            lines.append(blk['Text'])\n",
    "\n",
    "textract_results_filename = 'textract-results.txt'\n",
    "with open(f'./results/{textract_results_filename}', 'w') as fd:\n",
    "    for line in lines:\n",
    "        fd.write(f'{line}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Comprehend\n",
    "Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to discover insights from text. The service provides APIs for Keyphrase Extraction, Sentiment Analysis, Entity Recognition, Topic Modeling, and Language Detection so you can easily integrate natural language processing into your applications. The following cells will walk through several examples of how to use the API.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrase Extraction\n",
    "Use Amazon Comprehend to extract Key Phrases in the text from the Textract analysis.  \n",
    "The input is a UTF-8 text string that must contain fewer that 5,000 bytes of UTF-8 encoded characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the comprehend boto3 client (from the existing boto3 session)\n",
    "comp_client = session.client('comprehend')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases = []\n",
    "\n",
    "for line in lines:     \n",
    "    response = comp_client.detect_key_phrases(Text=line, LanguageCode='en')\n",
    "    for kp in response['KeyPhrases']:\n",
    "        if kp['Text'] not in key_phrases:\n",
    "            key_phrases.append(kp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentiment Analysis\n",
    "Use Amazon Comprehend to determine the Sentiment of each line of text from the Textract analysis.\n",
    "* POSITIVE, NEUTRAL, NEGATIVE, MIXED\n",
    "\n",
    "The input is a UTF-8 text string that must contain fewer that 5,000 bytes of UTF-8 encoded characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "    \n",
    "for line in lines:\n",
    "    response = comp_client.detect_sentiment(Text=line, LanguageCode='en')\n",
    "    sentiments.append(response['Sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Entity Recognition\n",
    "Use Amazon Comprehend to detect Entities in the text from the Textract analysis.  \n",
    "What are the type of Entities?\n",
    "* PERSON, ORGANIZATION, DATE, QUANTITY, LOCATION, TITLE, COMMERCIAL_ITEM, EVENT, OTHER\n",
    "\n",
    "The input is a UTF-8 text string that must contain fewer that 5,000 bytes of UTF-8 encoded characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entities = []\n",
    "for line in lines:\n",
    "    response = comp_client.detect_entities(Text=line, LanguageCode='en')\n",
    "    entities.append(response['Entities'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PII Entity Recognition\n",
    "Use Amazon Comprehend to detect PII Entities in the text from the Textract analysis.  \n",
    "What are the types of PII Entities?  \n",
    "* NAME, DATE-TIME, ADDRESS, USERNAME, URL, EMAIL, PHONE, CREDIT-DEBIT-EXPIRY, PASSWORD, AGE\n",
    "\n",
    "The input is a UTF-8 text string that must contain fewer that 5,000 bytes of UTF-8 encoded characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_entities = []\n",
    "for line in lines:\n",
    "    response = comp_client.detect_pii_entities(Text=line, LanguageCode='en')\n",
    "    pii_entities.append(response['Entities'])    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pii_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Topic Modeling\n",
    "Use Amazon Comprehend to extract Topics in the text from the Textract analysis.  \n",
    "\n",
    "In this example, we are running the analysis as an asynchronous job, so the results are stored in a file in the S3 bucket we specify.  \n",
    "This analysis may take up to 10 minutes to run.  \n",
    "\n",
    "The output results are two files:  \n",
    "*topic_terms.csv:*  A list of topics in the collection. For each topic, the list includes the top terms by topic according to their weight.  \n",
    "*doc-topics.csv:*   Lists the documents associated with a topic and the proportion of the document that is concerned with the topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the file to be analyzed into the s3 bucket\n",
    "# in this example, this file is the results from running textract on a pdf\n",
    "s3dest = f's3://{s3bucket}/{s3prefix}/{textract_results_filename}'\n",
    "!aws s3 cp ./results/$textract_results_filename $s3dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the Amazon Comprehend Topics Analysis job\n",
    "# create a unique Job Name\n",
    "JobName = 'MyJobName-%d' % (time.time())\n",
    "\n",
    "request = {\n",
    "   \"ClientRequestToken\": \"string\",\n",
    "   \"DataAccessRoleArn\": comprehend_role,\n",
    "   \"InputDataConfig\": { \n",
    "      \"InputFormat\": \"ONE_DOC_PER_FILE\",\n",
    "      \"S3Uri\": s3dest\n",
    "   },\n",
    "   \"JobName\": JobName,\n",
    "   \"OutputDataConfig\": { \n",
    "      \"S3Uri\": f's3://{s3bucket}/{s3prefix}/'\n",
    "   }\n",
    "}\n",
    "\n",
    "# create the comprehend analysis job\n",
    "response = comp_client.start_topics_detection_job(**request)\n",
    "JobId = response['JobId']\n",
    "print(JobId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comp_client.describe_topics_detection_job(JobId=JobId)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the comprehend analysis results are in the s3 bucket, full path is S3Uri\n",
    "s3uri = response['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "basename = os.path.basename(s3uri)\n",
    "\n",
    "# copy the 'output.tar.gz' file from the s3 bucket to the results folder\n",
    "!aws s3 cp $s3uri $results_dir\n",
    "\n",
    "# extract the contents of this tarball, which are two files: topic-terms.csv, doc-topics.csv\n",
    "!(cd $results_dir; tar xzf $basename)\n",
    "!(cd $results_dir; rm -f $basename)\n",
    "\n",
    "print('See the following files:')\n",
    "!ls -l $results_dir/topic-terms.csv\n",
    "!ls -l $results_dir/doc-topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

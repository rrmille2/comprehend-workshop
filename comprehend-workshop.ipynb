{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Comprehend\n",
    "#### Extract valuable insights from text within documents\n",
    "\n",
    "## Contents \n",
    "1. [Setup](#Setup)\n",
    "1. [IAM Roles and Permissions](#IAM)\n",
    "1. [Amazon Textract](#Textract)\n",
    "1. [Key Phrase Extraction](#KeyPhrases)\n",
    "1. [Sentiment Analysis](#Sentiment)\n",
    "1. [Entity Recognition](#Entity)\n",
    "1. [PII Entity Recognition](#PII)\n",
    "1. [PII Entity Recognition - Batch Mode](#PII-Batch)\n",
    "1. [Topic Modeling](#Topics)\n",
    "1. [Custom Classifiers](#Custom)\n",
    "\n",
    "#### Notes and Configuration\n",
    "* Kernel `Python 3 (Data Science)` works well with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"Setup\"></a>\n",
    "Set some variables that will be used throughout this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "sm_session = sagemaker.Session()\n",
    "\n",
    "s3bucket = sm_session.default_bucket()    \n",
    "s3prefix = 'comprehend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where the various analysis results files will be stored on the local file system of this SageMaker instance\n",
    "results_dir = './results'\n",
    "!mkdir -p $results_dir\n",
    "\n",
    "# this is the IAM Role that defines which permissions this SageMaker instance has\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "print('sagemaker execution role: ', sagemaker_role)\n",
    "print('s3 bucket:', s3bucket)\n",
    "print('s3 prefix:', s3prefix)\n",
    "print('region:', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to the ARN of the Role you created in the Textract Notebook\n",
    "#comprehend_role = 'arn:aws:iam::662559257807:role/myComprehendDataAccessRole'\n",
    "comprehend_role = '<enter your Comprehend Role created from the previous Textract workshop>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Textract <a name=\"Textract\"></a>\n",
    "Amazon Textract is a machine learning service that automatically extracts text, handwriting and data from scanned documents that goes beyond simple optical character recognition (OCR) to identify, understand, and extract data from forms and tables.  \n",
    "  \n",
    "In the next few cells the following steps will be performed:\n",
    "1. A specified PDF document will be uploaded to Amazon S3 to be analyzed by Amazon Textract.  \n",
    "1. The result of this analysis is a JSON file with each element containing details about a specific instance of text in the PDF.  \n",
    "1. This JSON file is copied from S3 to this local SageMaker instance.  \n",
    "1. The JSON file is then read and post-processed to produce a text file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Textract Job\n",
    "#textract_src_filename = 'amazon-press-release.png'\n",
    "#textract_src_filename = 'police-report.pdf'\n",
    "textract_src_filename = 'resume.pdf'\n",
    "\n",
    "# upload the source document to S3 for Textract to access\n",
    "!aws s3 cp data/$textract_src_filename s3://$s3bucket/$s3prefix/$textract_src_filename\n",
    "\n",
    "textract_client = session.client('textract')\n",
    "response = textract_client.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "    'S3Object': {\n",
    "        'Bucket': s3bucket,\n",
    "        'Name': f'{s3prefix}/{textract_src_filename}'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "JobId = response[\"JobId\"]\n",
    "print('JobId: %s' % (JobId))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = textract_client.get_document_text_detection(JobId=JobId)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while response['JobStatus'] != 'SUCCEEDED':\n",
    "    print('.', end='')\n",
    "    response = textract_client.get_document_text_detection(JobId=JobId)\n",
    "    time.sleep(5)\n",
    "print('done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "while(True):\n",
    "    pages.append(response)\n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "        response = textract_client.get_document_text_detection(JobId=JobId, NextToken=nextToken)\n",
    "\n",
    "    if nextToken == None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = ''\n",
    "\n",
    "# iterate through the Textract JSON response, looking for the LINE and WORD entries\n",
    "for page in pages:\n",
    "    for blk in page['Blocks']:\n",
    "        if blk['BlockType'] in ['LINE']:\n",
    "            fulltext = fulltext + '\\n' + blk['Text']\n",
    "\n",
    "textract_results_filename = 'textract-results.txt'\n",
    "with open(f'./results/{textract_results_filename}', 'w') as fd:\n",
    "    fd.write(f'{fulltext}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Comprehend\n",
    "Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to discover insights from text. The service provides APIs for Keyphrase Extraction, Sentiment Analysis, Entity Recognition, Topic Modeling, and Language Detection so you can easily integrate natural language processing into your applications. The following cells will walk through several examples of how to use the API.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrase Extraction <a name=\"KeyPhrases\"></a>\n",
    "Use Amazon Comprehend to extract Key Phrases in the text from the Textract analysis.  \n",
    "The input is a UTF-8 text string that must contain fewer that 5,000 bytes of UTF-8 encoded characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the comprehend boto3 client (from the existing boto3 session)\n",
    "comp_client = session.client('comprehend')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = comp_client.detect_key_phrases(Text=fulltext, LanguageCode='en')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentiment Analysis <a name=\"Sentiment\"></a>\n",
    "Use Amazon Comprehend to determine the Sentiment of each line of text from the Textract analysis.\n",
    "* POSITIVE, NEUTRAL, NEGATIVE, MIXED\n",
    "\n",
    "The input is a UTF-8 text string that must contain fewer that 5,000 bytes of UTF-8 encoded characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'That person looks happy today'\n",
    "response = comp_client.detect_sentiment(Text=sample_text, LanguageCode='en')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Entity Recognition <a name=\"Entity\"></a>\n",
    "Use Amazon Comprehend to detect Entities in the text from the Textract analysis.  \n",
    "What are the type of Entities?\n",
    "* PERSON, ORGANIZATION, DATE, QUANTITY, LOCATION, TITLE, COMMERCIAL_ITEM, EVENT, OTHER\n",
    "\n",
    "The input is a UTF-8 text string that must contain fewer that 5,000 bytes of UTF-8 encoded characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = comp_client.detect_entities(Text=fulltext, LanguageCode='en')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PII Entity Recognition <a name=\"PII\"></a>\n",
    "Use Amazon Comprehend to detect PII Entities in the text from the Textract analysis.  \n",
    "What are the types of PII Entities?  \n",
    "* NAME, DATE-TIME, ADDRESS, USERNAME, URL, EMAIL, PHONE, CREDIT-DEBIT-EXPIRY, PASSWORD, AGE\n",
    "\n",
    "In this example, we are looking at a string of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = comp_client.detect_pii_entities(Text=fulltext, LanguageCode='en')\n",
    "response           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PII Using Batch Processing <a name=\"PII-Batch\"></a>\n",
    "Use Amazon Comprehend to extract PII directly from a PDF document  \n",
    "\n",
    "In this example, we are running the analysis as an asynchronous job, so the results are stored in a file in the S3 bucket we specify.  \n",
    "This analysis may take up to 10 minutes to run.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the file to be analyzed into the s3 bucket\n",
    "# in this example, this file is the results from running textract on a pdf\n",
    "s3dest = f's3://{s3bucket}/{s3prefix}/{textract_results_filename}'\n",
    "!aws s3 cp ./results/$textract_results_filename $s3dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = comp_client.start_pii_entities_detection_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3dest,\n",
    "        'InputFormat': 'ONE_DOC_PER_FILE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': f's3://{s3bucket}/{s3prefix}'\n",
    "    },\n",
    "    Mode='ONLY_REDACTION',\n",
    "    RedactionConfig={\n",
    "        'PiiEntityTypes': ['ALL'],\n",
    "        'MaskMode': 'REPLACE_WITH_PII_ENTITY_TYPE',\n",
    "        'MaskCharacter': '*'\n",
    "    },\n",
    "    DataAccessRoleArn=comprehend_role,\n",
    "    #JobName='string',\n",
    "    LanguageCode='en'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobId = request['JobId']\n",
    "while True:\n",
    "    response = comp_client.describe_pii_entities_detection_job(JobId=JobId)\n",
    "    if response['PiiEntitiesDetectionJobProperties']['JobStatus'] != 'IN_PROGRESS':\n",
    "        print('')\n",
    "        break\n",
    "    print('.', end='')\n",
    "    time.sleep(5)\n",
    "    \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Topic Modeling <a name=\"Topics\"></a>\n",
    "Use Amazon Comprehend to extract Topics in the text from the Textract analysis.  \n",
    "\n",
    "In this example, we are running the analysis as an asynchronous job, so the results are stored in a file in the S3 bucket we specify.  \n",
    "This analysis may take up to 10 minutes to run.  \n",
    "\n",
    "The output results are two files:  \n",
    "*topic_terms.csv:*  A list of topics in the collection. For each topic, the list includes the top terms by topic according to their weight.  \n",
    "*doc-topics.csv:*   Lists the documents associated with a topic and the proportion of the document that is concerned with the topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the Amazon Comprehend Topics Analysis job\n",
    "request = comp_client.start_topics_detection_job(\n",
    "    InputDataConfig = { \n",
    "      \"InputFormat\": \"ONE_DOC_PER_FILE\",\n",
    "      \"S3Uri\": s3dest\n",
    "    },\n",
    "    OutputDataConfig = { \n",
    "      \"S3Uri\": f's3://{s3bucket}/{s3prefix}/'\n",
    "    },\n",
    "    DataAccessRoleArn = comprehend_role\n",
    ")\n",
    "\n",
    "JobId = request['JobId']\n",
    "print(JobId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while True:\n",
    "    response = comp_client.describe_topics_detection_job(JobId=JobId)\n",
    "    if response['TopicsDetectionJobProperties']['JobStatus'] != 'IN_PROGRESS':\n",
    "        print('')\n",
    "        break\n",
    "    print('.', end='')\n",
    "    time.sleep(10)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the comprehend analysis results are in the s3 bucket, full path is S3Uri\n",
    "s3uri = response['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "basename = os.path.basename(s3uri)\n",
    "\n",
    "# copy the 'output.tar.gz' file from the s3 bucket to the results folder\n",
    "!aws s3 cp $s3uri $results_dir\n",
    "\n",
    "# extract the contents of this tarball, which are two files: topic-terms.csv, doc-topics.csv\n",
    "!(cd $results_dir; tar xzf $basename)\n",
    "!(cd $results_dir; rm -f $basename)\n",
    "\n",
    "print('See the following files:')\n",
    "!ls -l $results_dir/topic-terms.csv\n",
    "!ls -l $results_dir/doc-topics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Comprehend - Custom Classifiers <a name=\"Custom\"></a>\n",
    "Use Amazon Comprehend to label or classify documents. This functionality gives you the ability to perform document classifications that are unique to your business.\n",
    "\n",
    "In this example, we are going to label some resumes.\n",
    "\n",
    "First, upload the resume data to S3 so we can train our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/resumes.csv')\n",
    "\n",
    "# use 90% of our data for training and the remainder for testing\n",
    "df_train = df.sample(frac = 0.9)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "df_train.to_csv('./data/train.csv', index=False)\n",
    "df_test.to_csv('./data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3dest = f's3://{s3bucket}/{s3prefix}/data/train.csv'\n",
    "!aws s3 cp data/train.csv $s3dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create (train) the Custom Classifier\n",
    "This step takes a long time to run (almost two hours) on the *resumes.csv* data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3results = f's3://{s3bucket}/{s3prefix}/custom/'\n",
    "\n",
    "request = comp_client.create_document_classifier(\n",
    "    DocumentClassifierName='myComprehendClassifier',\n",
    "    DataAccessRoleArn=comprehend_role,\n",
    "    InputDataConfig={\n",
    "        'DataFormat': 'COMPREHEND_CSV',\n",
    "        'S3Uri': s3dest,\n",
    "        'LabelDelimiter': '|',\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3results,\n",
    "    },\n",
    "    LanguageCode='en',\n",
    "    Mode='MULTI_LABEL',\n",
    "    VersionName='v1'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while True:\n",
    "    response = comp_client.describe_document_classifier(DocumentClassifierArn=request['DocumentClassifierArn'])\n",
    "    status = response[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    if status in ['TRAINED', 'FAILED']:\n",
    "        print('')\n",
    "        break\n",
    "    print('.', end='')\n",
    "    time.sleep(10)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take our sample resume data and run inferencing on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3dest = f's3://{s3bucket}/{s3prefix}/data/test.csv'\n",
    "!aws s3 cp ./data/test.csv $s3dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Classification Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arn = response['DocumentClassifierProperties']['DocumentClassifierArn']\n",
    "request = comp_client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': f's3://{s3bucket}/{s3prefix}/data/test.csv',\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': f's3://{s3bucket}/{s3prefix}/inf-results/'\n",
    "    },\n",
    "    DataAccessRoleArn = comprehend_role,\n",
    "    DocumentClassifierArn = arn\n",
    ")\n",
    "\n",
    "request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the Classification Job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while True:\n",
    "    response = comp_client.describe_document_classification_job(JobId=request['JobId'])\n",
    "    status = response[\"DocumentClassificationJobProperties\"][\"JobStatus\"]\n",
    "    if status != 'IN_PROGRESS':\n",
    "        print('')\n",
    "        break\n",
    "    print('.', end='')\n",
    "    time.sleep(10)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3file = response['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3file results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = os.path.basename(s3uri)\n",
    "!(cd results; tar xvzf $basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./results/predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 ./data/test.csv | tail -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
